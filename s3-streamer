#!/usr/bin/python3

import argparse
import codecs
import json
import locale
import mimetypes
import os
import select
import subprocess
import tempfile
import textwrap
import time
import urllib.parse

from bots.lib import s3


class Destination:
    def has(self, filename):
        raise NotImplementedError

    def write(self, filename, data):
        raise NotImplementedError

    def delete(self, filenames):
        raise NotImplementedError


class LocalDestination(Destination):
    def __init__(self, directory):
        self.directory = directory

    def path(self, filename):
        return os.path.join(self.directory, filename)

    def has(self, filename):
        return os.path.exists(self.path(filename))

    def write(self, filename, data):
        print(f'Write {self.path(filename)}')
        with open(self.path(filename), 'wb+') as file:
            file.write(data)

    def delete(self, filenames):
        for filename in filenames:
            print(f'Delete {self.path(filename)}')
            os.unlink(self.path(filename))


class S3Destination(Destination):
    def __init__(self, directory):
        self.directory = directory.rstrip('/') + '/'

    def url(self, filename):
        return urllib.parse.urlparse(self.directory + filename)

    def has(self, filename):
        raise NotImplementedError('use Index')

    def write(self, filename, data):
        headers = {
            'Content-Type': mimetypes.guess_type(filename)[0] or 'text/plain',
            s3.ACL: s3.PUBLIC
        }
        with s3.urlopen(self.url(filename), data=data, method='PUT', headers=headers) as response:
            print(response.status, filename)

    def delete(self, filenames):
        # to do: multi-object delete API
        for filename in filenames:
            with s3.urlopen(self.url(filename), method='DELETE') as response:
                print(response)


class Index(Destination):
    def __init__(self, destination, filename='index.html'):
        self.destination = destination
        self.filename = filename
        self.files = set()
        self.dirty = True

    def has(self, filename):
        return filename in self.files

    def write(self, filename, data):
        self.destination.write(filename, data)
        self.files.add(filename)
        self.dirty = True

    def delete(self, filenames):
        self.destination.delete(self.destination, filenames)
        self.files.difference_update(filenames)
        self.dirty = True

    def sync(self):
        if self.dirty:
            self.destination.write(self.filename, textwrap.dedent('''
                <html>
                  <body>
                    <h1>Directory listing for /</h1>
                    <hr>
                    <ul>''' + ''.join(f'''
                      <li><a href={f}>{f}</a></li> ''' for f in self.files) + '''
                    </ul>
                  </body>
                </html>
                ''').encode('utf-8'))
            self.dirty = False


class AttachmentsDirectory:
    def __init__(self, destination, local_directory):
        self.destination = destination
        self.path = local_directory

    def scan(self):
        for entry in os.scandir(self.path):
            if not self.destination.has(entry.name) and entry.is_file(follow_symlinks=False):
                with open(entry.path, 'rb') as file:
                    data = file.read()
                self.destination.write(entry.name, data)


class ChunkedUploader:
    SIZE_LIMIT = 1000000  # 1MB
    TIME_LIMIT = 1        # 1s

    def __init__(self, destination, filename):
        self.suffixes = {'chunks'}
        self.chunks = []
        self.destination = destination
        self.filename = filename
        self.pending = b''
        self.send_at = 0  # Send the first write immediately

    def append_block(self, block):
        self.chunks.append([block])

        # 2048 algorithm.
        #
        # This can be changed to merge more or less often, or to never merge at
        # all. The only restriction is that it may only ever update the last
        # item in the list.
        while len(self.chunks) > 1 and len(self.chunks[-1]) == len(self.chunks[-2]):
            last = self.chunks.pop()
            second_last = self.chunks.pop()
            self.chunks.append(second_last + last)

        # Now we figure out how to send that last item.
        # Let's keep the client dumb: it doesn't need to know about blocks: only bytes.
        chunk_sizes = [sum(len(block) for block in chunk) for chunk in self.chunks]

        if chunk_sizes:
            last_chunk_start = sum(chunk_sizes[:-1])
            last_chunk_end = last_chunk_start + chunk_sizes[-1]
            last_chunk_suffix = f'{last_chunk_start}-{last_chunk_end}'
            self.destination.write(f'{self.filename}.{last_chunk_suffix}', b''.join(self.chunks[-1]))
            self.suffixes.add(last_chunk_suffix)

        self.destination.write(f'{self.filename}.chunks', json.dumps(chunk_sizes).encode('ascii'))

    def write(self, data):
        self.pending += data.encode('utf-8')

        if self.pending:
            now = time.monotonic()

            if self.send_at is None:
                self.send_at = now + ChunkedUploader.TIME_LIMIT

            if now >= self.send_at or len(self.pending) >= ChunkedUploader.SIZE_LIMIT:
                self.append_block(self.pending)
                self.send_at = None
                self.pending = b''

    def finish(self, index, data=''):
        self.chunks.append([self.pending + data.encode('utf-8')])
        total = b''.join(b''.join(block for block in chunk) for chunk in self.chunks)
        index.write(self.filename, total)

        # If the client ever sees a 404, it knows that the streaming is over.
        self.destination.delete([f'{self.filename}.{suffix}' for suffix in self.suffixes])

class StreamReader:
    def __init__(self, file, encoding=None):
        self.codec = codecs.getincrementaldecoder(encoding or locale.getpreferredencoding())()
        self.file = file

    def read(self):
        eof = False
        data = b''

        while not eof and select.select([self.file.fileno()], [], [], 0)[0]:
            read = self.file.read1(100000)
            if not read:
                eof = True
            data += read

        return self.codec.decode(data, final=eof)

def main():
    parser = argparse.ArgumentParser()

    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--s3', help="Write to the given S3 URL")
    group.add_argument('--directory', help="Write to the named local directory")
    parser.add_argument('cmd', nargs='+', help="Command to stream the output of")
    args = parser.parse_args()

    if args.s3:
        destination = S3Destination(args.s3)
    elif args.directory:
        destination = LocalDestination(args.directory)
    else:
        raise AssertionError

    with tempfile.TemporaryDirectory() as tmpdir:
        index = Index(destination)
        attachments_directory = AttachmentsDirectory(index, tmpdir)
        log_uploader = ChunkedUploader(destination, 'log')

        with subprocess.Popen(args.cmd, env=dict(os.environ, TEST_ATTACHMENTS=tmpdir),
                              stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
                              stdin=subprocess.DEVNULL) as process:
            # We want non-blocking reads so that we can send attachments and
            # flush pending chunked data in case the output stalls.
            #os.set_blocking(process.stdout.fileno(), False)

            # Send the static files to start
            AttachmentsDirectory(index, 'static').scan()
            log = StreamReader(process.stdout)

            while process.poll() is None:
                # Order is important: read the log, upload attachments, send the log
                # The idea is that attachments should be present before the log that mentions them
                print('reading')
                data = log.read()
                print(data)

                attachments_directory.scan()

                log_uploader.write(data)
                index.sync()

                time.sleep(1)

            # One last time...
            data = log.read()
            print('final', data.__class__)
            attachments_directory.scan()
            log_uploader.finish(index, data)
            index.sync()


if __name__ == '__main__':
    main()
