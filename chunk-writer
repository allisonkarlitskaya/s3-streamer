#!/usr/bin/python3

import json
import os
import time


class Destination:
    def write(self, filename, data):
        raise NotImplementedError

    def delete(self, filenames):
        raise NotImplementedError


class LocalDestination(Destination):
    def __init__(self, directory):
        self.directory = directory

    def path(self, filename):
        return os.path.join(self.directory, filename)

    def write(self, filename, data):
        print(f'Write {self.path(filename)}')
        with open(self.path(filename), 'wb+') as file:
            file.write(data)

    def delete(self, filenames):
        for filename in filenames:
            print(f'Delete {self.path(filename)}')
            os.unlink(self.path(filename))


class S3Destination(Destination):
    pass


class ChunkedUploader:
    # /log
    # /log.chunks: [] or [123, 333] or 'null'
    # /log.0-123
    # /log.123-456

    def __init__(self, destination, filename):
        self.suffixes = set()
        self.chunks = []
        self.destination = destination
        self.filename = filename

    def append_block(self, block):
        self.chunks.append([block])

        # 2048 algorithm.
        #
        # This can be changed to merge more or less often, or to never merge at
        # all. The only restriction is that it may only ever update the last
        # item in the list.
        while len(self.chunks) > 1 and len(self.chunks[-1]) == len(self.chunks[-2]):
            last = self.chunks.pop()
            second_last = self.chunks.pop()
            self.chunks.append(second_last + last)

        # Now we figure out how to send that last item.
        # Let's keep the client dumb: it doesn't need to know about blocks: only bytes.
        chunk_sizes = [sum(len(block) for block in chunk) for chunk in self.chunks]
        last_chunk_start = sum(chunk_sizes[:-1])
        last_chunk_end = last_chunk_start + chunk_sizes[-1]
        last_chunk_suffix = f'{last_chunk_start}-{last_chunk_end}'

        self.destination.write(f'{self.filename}.{last_chunk_suffix}', b''.join(self.chunks[-1]))
        self.destination.write(f'{self.filename}.chunks', json.dumps(chunk_sizes).encode('ascii'))
        self.suffixes.add(last_chunk_suffix)

    def finish(self, block = b''):
        self.chunks.append([block])
        total = b''.join(b''.join(block for block in chunk) for chunk in self.chunks)
        self.destination.write(self.filename, total)
        self.destination.write(f'{self.filename}.chunks', b'null')

        self.destination.delete([f'{self.filename}.{suffix}' for suffix in self.suffixes])


uploader = ChunkedUploader(LocalDestination('srv'), 'log')
for i in range(60):
    uploader.append_block(f'{i}\n'.encode('ascii'))
    time.sleep(1)
uploader.finish()
